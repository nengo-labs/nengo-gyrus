from functools import wraps
from weakref import WeakKeyDictionary

import numpy as np

import nengo
from nengo.utils.numpy import is_iterable

from gyrus.nengo_helpers import is_probeable


class NengoSimulatorMixin:
    """Provides methods for generating, simulating, and probing Nengo models.

    Mixing in this class allows any operator, including all of its input dependencies,
    and so on (recursively) to be run as a Nengo model. This assumes that the instance
    has an ``input_ops`` attribute which is a tuple of objects that also mix in this
    type, and that each such object has a ``generate`` method.

    There are two ways to create Nengo models through this mixin:

      1. The first is to simply call the ``run`` method, which creates a new Nengo
         network context, generates all of the Nengo objects, sets up probes, builds
         and simulates the model, and then returns the probed data. The is the simplest
         approach if one is working entirely in Gyrus and is primarily concerned with
         obtaining the output(s) of the simulation.

      2. The second is to call the ``make`` method, which adds any missing Nengo objects
         to the inner-most Nengo network context. The method returns the output node of
         the operator (or a recursive structure of nodes in the case of a Fold). This
         allows a user to work within Nengo as usual but augment existing Nengo models
         using Gyrus.

    The graph of operators is a directed acylic graph (DAG) by design (i.e., functional
    programming model with immutable operators) and so the main consideration is that
    the same operator should not be built twice into the same Nengo network context.
    This is achieved by defining a Nengo network -> cache mapping that caches any
    operators that have already been built into the inner-most Nengo network.
    As a result, one must be aware that making the same operator within two different
    networks will result in the operator's entire DAG being regenerated within each
    respective Nengo network context -- even if one is a subnetwork of the other!
    If this is not the desired behaviour, then the network that is to be used to
    determine the cache can be passed to the ``make`` method. This only changes the
    logic for determining whether or not to regenerate an operator; the Nengo objects
    generated by the operator are still built into the current Nengo network context.

    In the simplest case of a single output, the returned probed data is the same as
    usual: ``sim.data[p]`` where ``p = nengo.Probe(obj)`` and ``obj`` is the Nengo Node
    produced by the operator.

    More generally, when probing a Fold of operators, the probed data is a recursively
    nested tuple that mirrors the structure of the Fold. Each element of the Fold
    reduces to the case of probing a single output.
    """

    # Set defaults for automatic labelling via self.label -> str(self).
    label_max_depth = 2
    label_max_width = 1

    _network_to_cache = WeakKeyDictionary()

    @property
    def label(self):
        """Defines the Nengo label for the result of generate, if applicable."""
        return self.__str__(
            max_depth=self.label_max_depth, max_width=self.label_max_width
        )

    @classmethod
    def probeable(cls, x):
        """Returns True iff the probe method can handle x."""
        return is_iterable(x) or is_probeable(x)

    @classmethod
    def probe(cls, x):
        """Returns a function that consumes a simulator and produces probe data."""
        # Probes are created eagerly, but the actual probing is done lazily
        # through recursive calls that inject the simulator object.
        if is_iterable(x):
            p_funcs = tuple(cls.probe(xi) for xi in x)
            return lambda sim: tuple(p_func(sim) for p_func in p_funcs)
        p = nengo.Probe(x)
        return lambda sim: sim.data[p]

    def _make(self, cache):
        """Generates operator's graph within the current Nengo network context."""
        # Ensures that if two ops depend on the same op then it doesn't get generated
        # twice (otherwise would create the same ensembles many times over potentially).
        if self in cache:
            if cache[self] is None:
                # This should not be possible since operators are meant to be immutable
                # and so operators can only depend on other operators that have already
                # been initialized before this one.
                raise RuntimeError(
                    "infinite cycle in input operator dependencies detected"
                )
            return cache[self]
        cache[self] = None  # used to detect infinite recursion
        input_nodes = tuple(op._make(cache) for op in self.input_ops)
        output = self.generate(*input_nodes)
        if not self.probeable(output):
            raise ValueError(
                f"generate is expected to return a probeable object, but got: {output}"
            )
        # Automatically label the output if it has a label attribute that is None.
        if getattr(output, "label", "") is None:
            output.label = self.label
        cache[self] = output
        return output

    def make(self, network=None):
        """Generates operator's graph within the current Nengo network context."""
        # The network parameter does not determine which network the Nengo objects are
        # built into. Rather, it determines which cache to use for the result of
        # operator generation in _make.
        if not len(nengo.Network.context):
            raise RuntimeError(
                "make method is meant to be invoked within an existing nengo.Network "
                "context; did you mean to use the run method instead?"
            )
        if network is None:
            network = nengo.Network.context[-1]
        cache = self._network_to_cache.setdefault(network, {})
        return self._make(cache=cache)

    def run(self, t, dt=0.001, label=None, seed=None, simulator=nengo.Simulator):
        """Generates a Nengo network, then probes, simulates, and returns its data."""
        # Note: Each call to run will regenerate the entire operator graph. Should use
        # the make method if it is desired to reuse the same objects within a single
        # Network.
        if len(nengo.Network.context):
            raise RuntimeError(
                "run method is not meant to be invoked within an existing "
                "nengo.Network context; did you mean to use the make method instead?"
            )

        with nengo.Network(label=label, seed=seed) as network:
            assert network is nengo.Network.context[-1]
            accessor = self.probe(self.make())

        with simulator(network, dt=dt) as sim:
            sim.run(t)

        return accessor(sim)


class RegisterOperatorsMixin:
    """Instrumentation to overload NumPy ufuncs, methods, and array functions.

    There are three mechanisms by which implementations, that are defined elsewhere,
    are executed by mixing in this class:

      1. Registering NumPy ufuncs. Decorating a function with
         ``@cls.register_ufunc(ufunc)`` where ``ufunc`` is a NumPy ufunc such as
         ``np.add`` will delegate handling of said ufunc, applied to any operator
         that is an instance of type ``cls``, to the decorated function. This works
         by defining the special NumPy method, ``__array_ufunc__``. Dunder methods
         such as ``__add__`` and ``__radd__`` delegate to these NumPy ufuncs.
         Inplace operations such as ``__iadd__`` are intentionally not supported
         as they would break the immutability of Gyrus operators.

         More information:
          - https://numpy.org/devdocs/reference/generated/numpy.lib.mixins.NDArrayOperatorsMixin.html
          - https://numpy.org/doc/1.13/neps/ufunc-overrides.htm

      2. Registering methods. Decorating a function with
         ``@cls.register_method(method_name)`` will cause ``cls.method_name(...)``
         to invoke the decorated function with the instance of ``cls`` as the
         first argument to the function. For example, ``op.decode(...)`` will invoke
         ``decode(op, ...)`` where ``decode`` is the decorated function and ``op``
         is an instance of ``cls``.

      3. Registering NumPy array functions. Decorating a NumPy array function, such
         as ``np.sum``, with ``@cls.register_method(method_name)``, has the same
         effect as the second mechanism, but in addition overrides the behaviour of that
         NumPy array function. The type ``cls`` must define the special NumPy method,
         ``__array_function__`` which determines how the array function is handled.
         Currently, ``Fold`` is the only type of operator that defines this method,
         and it does so by delegating them to the underlying NumPy array functions
         with appropriate type casting on the arguments and return values.

         More information:
          - https://numpy.org/devdocs/reference/arrays.classes.html#numpy.class.__array_function__
    """  # noqa: E501

    _registered_ufuncs = {}  # used by __array_ufunc__
    _registered_methods = {}  # used by __array_function__

    @classmethod
    def register_method(cls, method_name):
        """Register a function as a method that takes self as its first argument."""

        def decorator(dest_cls):
            if hasattr(cls, method_name):
                raise ValueError(
                    f"{cls} class already has already method with name: '{method_name}'"
                )

            # Methods such as __getitem__ need to be directly monkeypatched onto the
            # class as doing op[slice] directly invokes __getitem__ rather than going
            # through __getattr__ or __getattribute__.
            @wraps(dest_cls)
            def wrapper(*args, **kwargs):
                # This looks redundant but it's needed because wrapper will become
                # a method attached to cls, while dest_cls is a class (not a method)
                # that just happens to accept the self of cls as its first argument.
                return dest_cls(*args, **kwargs)

            setattr(cls, method_name, wrapper)
            cls._registered_methods[dest_cls] = method_name
            return dest_cls  # currently leave the target class untouched

        return decorator

    @classmethod
    def register_ufunc(cls, ufunc, method="__call__"):
        """Register a function as a NumPy ufunc handler for this type."""
        key = (ufunc, method)

        def decorator(function):
            if key in cls._registered_ufuncs:
                raise ValueError(f"{cls} class already implements {ufunc}.{method}")
            cls._registered_ufuncs[key] = function
            return function

        return decorator

    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
        """Handles NumPy ufuncs by delegating them to registered functions."""
        key = (ufunc, method)
        if key in self._registered_ufuncs:
            return self._registered_ufuncs[key](*inputs, **kwargs)
        return NotImplemented

    # Following methods are a subset of np.lib.mixins.NDArrayOperatorsMixin.

    def __add__(self, other):
        return np.add(self, other)

    def __radd__(self, other):
        return np.add(other, self)

    def __sub__(self, other):
        return np.subtract(self, other)

    def __rsub__(self, other):
        return np.subtract(other, self)

    def __neg__(self):
        return np.negative(self)

    def __mul__(self, other):
        return np.multiply(self, other)

    def __rmul__(self, other):
        return np.multiply(other, self)

    def __truediv__(self, other):
        return np.true_divide(self, other)

    def __rtruediv__(self, other):
        return np.true_divide(other, self)

    def __matmul__(self, other):
        return np.matmul(self, other)

    def __rmatmul__(self, other):
        return np.matmul(other, self)

    def __pow__(self, other):
        return np.power(self, other)

    def __rpow__(self, other):
        return np.power(other, self)
