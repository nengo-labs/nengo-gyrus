{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverting Matrices with Spiking Neurons\n",
    "\n",
    "Suppose we'd like to build a Spiking Neural Network (SNN) that can invert some input\n",
    "matrix, purely by using spiking nonlinearities. It is not at all obvious how to do\n",
    "so using existing software libraries. Gyrus makes this surprisingly simple, accurate,\n",
    "and scalable, once the problem is approached from the perspective of expressing the\n",
    "solution as a suitable algorithm in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import nengo\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib.colors import Normalize\n",
    "from nengo.utils.numpy import rms\n",
    "from nengo_gui.jupyter import InlineGUI\n",
    "\n",
    "import gyrus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=0)\n",
    "A = rng.randn(2, 2)  # test case\n",
    "print(\"Condition number:\", np.linalg.cond(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nengo Approach\n",
    "\n",
    "Even in the 1x1 case this task is quite challenging, as it reduces to trying to\n",
    "approximate $f(x) = 1 / x$, which is an unnatural function to fit using the tuning\n",
    "curves of a Nengo ensemble.\n",
    "\n",
    "Nevertheless, we may attempt this in Nengo by decoding the matrix inverse from an\n",
    "ensemble, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use evaluation points with small condition numbers prevent the decoders\n",
    "# from trying to fit arbitrarily large inverses.\n",
    "max_cond = 10\n",
    "eval_points = []\n",
    "for Atrain in rng.randn(5000, *A.shape):\n",
    "    if np.linalg.cond(Atrain) < max_cond:\n",
    "        eval_points.append(Atrain.flatten())\n",
    "radius = np.percentile(np.linalg.norm(eval_points, axis=-1), q=90)\n",
    "\n",
    "\n",
    "def function(x):\n",
    "    if np.all(x == 0):\n",
    "        return x\n",
    "    return np.linalg.inv(x.reshape(A.shape)).flatten()\n",
    "\n",
    "\n",
    "with nengo.Network(seed=0) as model:\n",
    "    stim = nengo.Node(A.flatten())\n",
    "\n",
    "    x = nengo.Ensemble(\n",
    "        n_neurons=2500,\n",
    "        dimensions=A.size,\n",
    "        eval_points=eval_points,\n",
    "        radius=radius,\n",
    "    )\n",
    "\n",
    "    Ainv_hat = nengo.Node(size_in=A.size)\n",
    "\n",
    "    nengo.Connection(stim, x, synapse=None)\n",
    "    nengo.Connection(x, Ainv_hat, function=function, synapse=None)\n",
    "\n",
    "    p = nengo.Probe(Ainv_hat, synapse=0.2)\n",
    "\n",
    "with nengo.Simulator(model) as sim:\n",
    "    sim.run(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(A, Ainv_hat):\n",
    "    Ainv = np.linalg.inv(A)\n",
    "    nrmse = rms(Ainv - Ainv_hat) / rms(Ainv)\n",
    "    print(f\"Normalized RMSE: {(100 * nrmse).round(1)}%\")\n",
    "\n",
    "    vlim = np.max(np.abs([Ainv, Ainv_hat]))\n",
    "    cmap = cm.get_cmap(\"bwr\")\n",
    "    im = cm.ScalarMappable(cmap=cmap, norm=Normalize(-vlim, vlim))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 3), sharey=True)\n",
    "    axes[0].set_title(\"Ground Truth ($A^{-1}$)\")\n",
    "    axes[0].imshow(Ainv, vmin=-vlim, vmax=vlim, cmap=cmap)\n",
    "    axes[1].set_title(r\"Approximation ($\\tilde{A}^{-1}$)\")\n",
    "    axes[1].imshow(Ainv_hat, vmin=-vlim, vmax=vlim, cmap=cmap)\n",
    "    axes[2].set_title(r\"Error ($A^{-1} - \\tilde{A}^{-1}$)\")\n",
    "    axes[2].imshow(Ainv - Ainv_hat, vmin=-vlim, vmax=vlim, cmap=cmap)\n",
    "    fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_simulation(A, data):\n",
    "    data = np.asarray(data).reshape(-1, A.size)\n",
    "    Ainv = np.linalg.inv(A).flatten()\n",
    "    plt.figure()\n",
    "    for i in range(A.size):\n",
    "        (line,) = plt.plot(data[:, i])\n",
    "        plt.hlines(Ainv[i], 0, len(data) - 1, color=line.get_color(), linestyle=\"--\")\n",
    "    plt.xlabel(\"Time-step\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_simulation(A, sim.data[p])\n",
    "evaluate(A, sim.data[p][-1].reshape(A.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look okay visually. But even with 2,500 neurons and fiddling with the\n",
    "``eval_points`` in order to get it to properly train, the normalized root mean-squared\n",
    "error (NRMSE) is around 20%. And it is not clear how to efficiently scale this up to\n",
    "larger matrices (try this again with a 3x3 matrix and see) or how to improve the\n",
    "accuracy.\n",
    "\n",
    "One might try to use NengoDL to backpropagate across a deeper network. But a simpler\n",
    "solution, that does not require any backpropagation at all, is to approach this as an\n",
    "iterative optimization problem, which we can do using NumPy (and thus Gyrus as well).\n",
    "As we will see, this algorithm is converted into a recurrent SNN that is cheap\n",
    "to train and scales efficiently to larger matrices.\n",
    "\n",
    "## NumPy Approach\n",
    "\n",
    "There are a few approaches to implementing matrix inversion as an algorithm using basic\n",
    "NumPy operations. One that fits very naturally with SNNs is to start with some initial\n",
    "guess of the inverse (e.g., all zeros), and then iteratively update it in the direction\n",
    "that minimizes the error. This is also known as gradient descent. In the simplest case,\n",
    "the iterative updates are Euler method updates of the gradient with respect to the\n",
    "mean-squared error in the current estimate.\n",
    "\n",
    "Specifically, if $M$ is the current guess of $A^{-1}$, then we would like for $MA = I$.\n",
    "Thus, the error that we wish to minimize is the mean-squared error, $\\sum (MA - I)^2$.\n",
    "Taking the gradient of this with respect to $M$ is $2 (MA - I)A^T$, which may look\n",
    "familiar from least-squares optimization. The NumPy algorithm is then to simply\n",
    "integrate this gradient, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(A, M):\n",
    "    \"\"\"Compute the gradient of M approximating inv(A).\"\"\"\n",
    "    I = np.eye(A.shape[0])\n",
    "    return 2 * (M @ A - I) @ A.T\n",
    "\n",
    "\n",
    "def numpy_inverse(A, M, dt, n_iter, verbose=False):\n",
    "    \"\"\"Compute the inverse of A by gradient descent from M.\"\"\"\n",
    "    data = []\n",
    "    for _ in range(n_iter):\n",
    "        if verbose:\n",
    "            data.append(M.copy())\n",
    "        M -= dt * gradient(A, M)\n",
    "\n",
    "    if verbose:\n",
    "        plot_simulation(A, data)\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "Ainv_hat = numpy_inverse(A=A, M=np.zeros_like(A), dt=0.05, n_iter=50, verbose=True)\n",
    "evaluate(A, Ainv_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us the correct answer (i.e., approximately 0.0% normalized RMSE). However,\n",
    "this is not using spiking neurons. Nevertheless, it is using primitive NumPy\n",
    "operations that we can automagically convert to Nengo using Gyrus.\n",
    "\n",
    "## Gyrus Approach - Initial\n",
    "\n",
    "The `gradient` function is already compatible with Gyrus without any modification. And\n",
    "so is the `numpy_inverse` function in fact! But, the `for` loop will essentially create\n",
    "`n_iter=50` layers of the exact same network. We can use the code as is, but it will be\n",
    "slow and not scale well to larger matrices or more iterations.\n",
    "\n",
    "The solution is to recognize that the `gyrus.integrate` operation is like a `for` loop;\n",
    "this operation rolls up some additive update into a recurrent network by integrating\n",
    "some given `integrand` over time. In this case, the integrand is just `-gradient(A, M)`\n",
    "with appropriate scaling by Nengo's default time-step of `1e-3`. Since we're using\n",
    "spiking neurons, we also filter the integral with a lowpass synapse to decode the final\n",
    "approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gyrus_inverse(A, M, dt, synapse=None):\n",
    "    \"\"\"Compute the inverse of A by gradient descent from M.\"\"\"\n",
    "    return M.integrate_fold(\n",
    "        integrand=lambda M: -dt * gradient(A, M) / 1e-3,\n",
    "        synapse=synapse,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = gyrus_inverse(\n",
    "    A=gyrus.stimuli(A).configure(\n",
    "        n_neurons=1000,\n",
    "        input_magnitude=2,\n",
    "        seed=0,\n",
    "    ),\n",
    "    M=gyrus.stimuli(np.zeros_like(A)),\n",
    "    dt=0.05,\n",
    ").filter(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.asarray(op.run(1)).squeeze(axis=-1)\n",
    "Ainv_hat = out[..., -1]\n",
    "\n",
    "plot_simulation(A, np.moveaxis(out, -1, 0))  # move time to first dimension\n",
    "evaluate(A, Ainv_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works quite a bit better, and all nonlinearities are implemented using spiking LIF\n",
    "neurons! Moreover, relatively little effort was required to build and try this out (only\n",
    "a few extra lines of code).\n",
    "\n",
    "Now let's see what this network looks like in the NengoGUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network() as model:\n",
    "    op.make()\n",
    "\n",
    "InlineGUI(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot going on there, but it's all just multiply operators (i.e., Nengo product\n",
    "networks) connected together with linear transforms in a very complicated but specific\n",
    "way. If one were to try to recreate this without using Gyrus, that is, by specifying\n",
    "each primitive Nengo object one statement at a time, the endeavour would not only be\n",
    "arduous, but the result would be difficult to understand and modify.\n",
    "\n",
    "## Gyrus Approach - Optimizing Spikes\n",
    "\n",
    "We can improve the efficiency of the SNN in terms of how many spikes are needed to\n",
    "obtain some level of accuracy by applying a number of advanced techniques:\n",
    "\n",
    "1. For each ensemble, use just a single `gyrus.Parabola()` response curve centered\n",
    "around 0, which is ideal for the Product network.\n",
    "2. Use the `nengo.solvers.Lstsq()` solver since regularization is no longer required\n",
    "with the above.\n",
    "3. To integrate the gradient over time, use the Keras `Adam` optimizer as a synapse\n",
    "(`gyrus.Adam`), instead of Euler's method. This also make the network significantly more\n",
    "robust to changes in hyperparameters.\n",
    "\n",
    "We demonstrate that this can scale by inverting a 5x5 matrix, and count the number of\n",
    "spikes required to achieve a given normalized RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go(A, dt, t, rates, synapse, output_synapse):\n",
    "    op = gyrus_inverse(\n",
    "        A=gyrus.stimuli(A).configure(n_neurons=1),\n",
    "        M=gyrus.stimuli(np.zeros_like(A)),\n",
    "        dt=dt,\n",
    "        synapse=synapse,\n",
    "    ).filter(output_synapse)\n",
    "\n",
    "    with nengo.Network() as model:\n",
    "        # RegularSpiking wrapper requires nengo>=3.1.0.\n",
    "        model.config[nengo.Ensemble].neuron_type = nengo.RegularSpiking(\n",
    "            gyrus.Parabola()\n",
    "        )\n",
    "        model.config[nengo.Ensemble].encoders = nengo.dists.Choice([[1]])\n",
    "        model.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
    "        model.config[nengo.Ensemble].max_rates = nengo.dists.Choice([rates])\n",
    "        model.config[nengo.Connection].solver = nengo.solvers.Lstsq()\n",
    "        accessor = gyrus.probe(op.make())\n",
    "\n",
    "        # Count spikes to analyze efficiency.\n",
    "        spike_counter = nengo.Node(size_in=1)\n",
    "        for ens in model.all_ensembles:\n",
    "            # Note: Spikes are scaled by 1 / dt in Nengo.\n",
    "            nengo.Connection(ens.neurons, spike_counter, transform=1e-3, synapse=None)\n",
    "        p_spike_count = nengo.Probe(spike_counter)\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        with nengo.Simulator(model, optimize=False) as sim:\n",
    "            sim.run(t)\n",
    "\n",
    "    return model, accessor(sim), sim.data[p_spike_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=0)\n",
    "A = rng.randn(5, 5)  # test case\n",
    "print(\"Condition number:\", np.linalg.cond(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data, spike_counts = go(\n",
    "    A=A,\n",
    "    dt=0.1,\n",
    "    t=2.5,\n",
    "    rates=1e3,\n",
    "    synapse=gyrus.Adam(1),\n",
    "    output_synapse=0.2,\n",
    ")\n",
    "out = np.asarray(data).squeeze(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ainv_hats = np.moveaxis(out, -1, 0)  # move time to first dimension\n",
    "Ainv_hat = out[..., -1]\n",
    "\n",
    "plot_simulation(A, Ainv_hats)\n",
    "evaluate(A, Ainv_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ainv = np.linalg.inv(A)\n",
    "nrmse = 100 * rms(Ainv[None, :] - Ainv_hats, axis=(1, 2)) / rms(Ainv)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(spike_counts), nrmse)\n",
    "# plt.yscale(\"log\")\n",
    "plt.xlabel(\"Total # Spikes\")\n",
    "plt.ylabel(\"Normalized RMSE (%)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"# Neurons: {model.n_neurons}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InlineGUI(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has 500 parabolic neurons in total, and inverts a 5x5 matrix with <0.5% NRMSE\n",
    "after generating on the order of a million spikes.\n",
    "\n",
    "Resources can be parallelized by increasing `n_neurons` and decreasing the `rates` of\n",
    "each neuron. Also, the NRMSE decreases as a function of the total number of spikes being\n",
    "generated (e.g., ~10% NRMSE is achieved after roughly half as many spikes are\n",
    "generated). Thus, we can dynamically trade between latency, energy, and precision, by\n",
    "dialing certain model parameters.\n",
    "\n",
    "In general, the ideal model settings will depend on the hardware, its supported neuron\n",
    "response curves, the energy budget, the timing requirements of an application, and the\n",
    "level of accuracy required downstream. These trade-offs are discussed in more detail in\n",
    "[A spike in performance: Training hybrid-spiking neural networks with quantized\n",
    "activation functions](https://arxiv.org/abs/2002.03553) and [Dynamical Systems in\n",
    "Spiking Neuromorphic Hardware](https://uwspace.uwaterloo.ca/handle/10012/14625)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
